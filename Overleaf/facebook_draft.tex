
\documentclass[12pt,letterpaper]{article}
\usepackage{adjustbox}
\usepackage[margin=1in]{geometry}  %set margin
\usepackage{booktabs}   % for pretty tables
\usepackage{url}  % makes nice looking urls
\usepackage{epstopdf}  % can't remember what this is for
\usepackage{appendix}
\usepackage{amsmath,amssymb}  % I think this is for pretty math? who knows...
\usepackage{amsthm}  % I think this is for pretty math? who knows...
\usepackage{pdflscape} % to put some pages in landscape
\usepackage{placeins} %allows FloatBarrier
\usepackage[pdftex]{graphicx}
\usepackage{setspace}  % package for line spacing https://www.overleaf.com/project/5f9731ab945e4b0001e21193
\onehalfspacing %set line spacing to 1.5

\usepackage{natbib}  % for citation hyperlinks
\usepackage{hyperref}  % for citation hyperlinks
\hypersetup{colorlinks,citecolor=blue}  % set citation hyperlinks blue
\pdfminorversion=6
\usepackage{tikz}
\usepackage[justification=centering]{caption}  % to use captionof to caption table/figure outside of \begin{table}
\usepackage{chngcntr} % to reset counter in appendix for figure/table numbering
\usepackage{subcaption} %to do subtables

\usepackage[utf8]{inputenc} % I don't know overleaf put this here

\usepackage{chngcntr}

\usepackage[parfill]{parskip} % style to have the paper use paragraph skips and no indents.  Feel free to change if you hate it
\usepackage[section]{placeins}
\usepackage{comment}
\usepackage{todonotes}
\usepackage{color}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{prop}[theorem]{Proposition}

%%%%%%%%%% Store some important variables %%%%%%%%%%

%Re: 12/7 email from AA to DD
\newcommand{\FullNFUSSurveySampleSize}{662}
\newcommand{\PYMKUSSurveySampleSize}{436}
\newcommand{\NFnPYMKIndiaSurveySampleSize}{200}
\newcommand{\ProlificSampleSize}{300}
\newcommand{\RecentInteractionsSampleSize}{104}
\newcommand{\StudyAboutSampleSize}{50}
\newcommand{\NoFurtherSampleSize}{122}
\newcommand{\numposts}{37,347}
\newcommand{\numhposts}{28,752}
\newcommand{\numrecs}{25,593}
\newcommand{\numinteract}{1,050}
\newcommand{\numhinteract}{908}
%%%%%%%%%% Title Page %%%%%%%%%%



\title{\vspace*{-.75in} A Clever Title about Facebook, Algorithms, and Bias\thanks{
We thank Devin Pope, Alex Imas, George Wu, Oleg Urminsky, Alex Todorov, Bernd Wittenbrink, Alex Koch, Josh Dean, Reid Hastie, Avner Strulov Shlain, Sam Hirshman, Jonathan Guryan and Jon Kleinberg for helpful comments. Seminar participants at Chicago Booth? Becky White, Bryan Baird, Amy Boonstra, and the team of RAs at CDR for research support. Tirtha Patel, Pavan Mamidi, and the team of RAs at CSBC at Ashoka University. We also thank the Harvard Decision Science Lab for providing RAs and research support
}  }

\author{ \vspace*{-.5in}%
\begin{tabular}[t]{cccc}
&  &  &  \\
Amanda Agan &  &  & Diag Davenport \\
\textit{Rutgers University} &  &  & \textit{University of Chicago} \\
\\
Jens Ludwig &  &  & Sendhil Mullainathan\\
\textit{University of Chicago} &  &  & \textit{University of Chicago} \\
&  &  &  \\
%\multicolumn{4}{c}{\textbf{PRELIMINARY AND INCOMPLETE - PLEASE DO NOT CIRCULATE}}  \\
&  &  &  \\
\end{tabular}%
}
\date{\today \vspace*{-0.15in}}


\begin{document}



\maketitle

%%%%%%%%%% Abstract %%%%%%%%%%

\begin{abstract}
\singlespacing
This paper empirically tests for in-group bias with two  algorithms on Facebook, the most widely used social media platform: Newsfeed (NF), which algorithmically orders friends' posts; and people-you-may-know (PYMK) which recommends potential new friends. These are examples of the growing use of algorithms to curate choice sets and rank items in ways that are intended to help us prioritize attention to items we are most likely to like. But there is often a wedge between human behavior and human preferences, as has been widely documented in psychology, particularly with respect to biases against people not like us. This means algorithms by their nature are forced to learn what we like from the most biased part of ourselves. We construct a simple model showing how algorithms can exacerbate human biases by curating choice sets that over-represent in-group members, which will be particularly pronounced for our less deliberate behaviors where in-group favoritism is most likely to creep in. The result is that in-groups are doubly favored: the algorithm includes more of the in-group into our choice set; and we continue to favor them in choosing from that set. Consistent with the predictions of the model, we find evidence of in-group bias with Newsfeed; for example in-group posts in the bottom quartile of the user's stated preference distribution have about the same average ranking in Newsfeed as out-group posts in the next quartile of the preference distribution. But we find no evidence of bias with PYMK, which involves more deliberation than interacting with a post. These results hold even when we control for user's self-reported preferences, such as interest in a post, or objective factors such as number of mutual friends. Finally, we demonstrate the generality of the underlying psychology and its interaction with algorithmic processes by replicating the results in a very different context, India, with a very different kind of in-group bias, religion.

\end{abstract}

\newpage

%%%%%%%%%% Begin Actual Paper %%%%%%%%%%

\section{Introduction}
%\todo[inline]{Todo}
%\todo{Todo}
We often have have many more options to choose from than we can easily consider. When the set of options is large, ranking algorithms may be useful to help curate our choice sets and prioritize our attention. For example, algorithms can sort our friend's new posts, tweets of people we follow, movie recommendations we might like, items we may want to purchase, and even which resumes to consider as part of the hiring process, which admissions file to read, etc. These algorithms are attractive because they learn people's preferences and can construct choice sets tailored to what we like. But their knowledge is indirect: algorithms can only learn from our behaviors, and must infer what we like from what we do. 

This can spell trouble. Psychologists (and behavioral economists?) have found again and again that our behaviors sometimes do not align with our preferences. This can happen for multiple potential reasons, for example cognitive constraints, complexity, conflicting preferences, use of heuristics, reliance on defaults, social pressure or norms, etc.\footnote{Social scientists across several disciplines including psychology, economics, sociology have documented various scenarios where there exists a gap between preferences and behavior, which can be caused by a myriad of reasons. Terminology can differ between papers---e.g. “experienced utility” and “decision utility”; or “true utility” and “decision utility”; or between “revealed” and “normative preferences” ( \citealt{kahneman1991economic}, \citealt{beshears2008preferences}, \citealt{bernheim2009beyond}). On the use of heuristics see e.g. \citealt{tversky1974judgment}, \citealt{shah2008heuristics}, \citealt{gigerenzer2008heuristics}, \citealt{bordalo2016stereotypes}. On wedges stemming from self-control see \citealt{mischel1989delay}, \citealt{kruglanski2002theory}. On dual-system processing see \citealt{kahneman2011thinking}. On the effect of memory see \citealt{stewart2006decision}. On the effect of attention see \citealt{gabaix2019behavioral}. On the effect of bounded rationality see Simon (1955?) and \citealt{conlisk1996bounded}. On the effect of framing and construal see ([insert construal and framing effects]). On the effect of automaticity see \citealt{dijksterhuis2006making}. On the effect of mistaken beliefs see \citealt{benjamin2019errors} [insert weight and evidence, conservatism, probability weighting from T\&K]. On the effect of social influence see \citealt{asch1951effects}, \citealt{milgram1978obedience}, \citealt{cialdini2004social}. On the effect of culture see \citealt{yamagishi2008preferences} and \citealt{henrich2010beyond}. On the effect of laws and institutions see \citealt{feagin1980discrimination}, \citealt{massey1993american}, \citealt{pager2008sociology}, \citealt{reskin2012race}, \citealt{small2020sociological}, \citealt{north1991institutions}. On the effect of nudges and choice architecture see \citealt{thaler2009nudge}.} Importantly, these discrepancies are neither constant nor universal; the wedge between preferences and choices can vary by context. For example, something as straightforward as spending more time deliberating can ensure the chosen action is more in line with our preference (CITE). 

\todo[inline]{DD: As I think about it more, economics is the disciplinary outsider in assuming that behavior reveals preferences in the first place. This review seems persuasive to an economics audience (maybe) because revealed preferences are so fundamental/axiomatic but since that fundamental assumption doesn't exist elsewhere, the review would seem odd to a non-economist...I think.

AA: Which Review? the whole footnote or the whole introduction? We can talk about it on call}


This preference-behavior wedge is particularly well-established for biases towards people not like us. We tend to favor people who belong to the same group as us (e.g. people of the same race, religion, gender, political party, etc.) Such favoritism creeps into our behaviors - and even more so into our less well-thought out behaviors. Given growing concerns in the research literature about algorithmic bias, the mechanism we hypothesize here gives us a new way to understand the source of some of this bias.\footnote{While this literature is vast, some key studies include \citealt{BarocasHardtNarayan-FairnessBook}, \citealt{BarocasSelbst2016}, \citealt{BolukbasiEtAl(16)}, \citealt{Boyd2012}, \citealt{CaliskanEtAl(17)}, \citealt{Chouldechova2017b}, \citealt{ChouldechovaRoth(20)}, \citealt{CorbettDaviesEtAl2017}, \citealt{CowgillTucker2019}, \citealt{Dwork2012}, \citealt{FusterEtAl(18)}, \citealt{GillisSpiess(19)}, \citealt{HardtPriceSrebro2016}, \citealt{HeidariEtAl(18)}, \citealt{HuChen(18)-WelfareFairness}, \citealt{KamishimaEtAl(11)}, \citealt{KamishimaEtAl(12)}, \citealt{KLMR(18)}, \citealt{KLMS(18)}, \citealt{KLMS(20)-PNAS}, \citealt{KM2019}, \citealt{LiptonEtAl(18)}, \citealt{LiuEtAl(18)}, \citealt{Mayson(18)}, \citealt{MenonWilliamson(18)}, \citealt{MitchellEtAl(19)}, \citealt{ObermeyerEtAl(19)}, \citealt{PleissEtAl(17)}, \citealt{RKM2017}, \citealt{RaghavanEtAl(19)}, \citealt{RambachanEtAl(20)-PP}, \citealt{RambachanEtAl(20)}, \citealt{RambachanRoth(19)-BiasInBiasOut}, \citealt{ZemelEtAl(13)}, and \citealt{ZafarEtAl(19)}. }

We build a simple model of how this piece of human psychology interacts with algorithmically constructed choice sets. We show that algorithms trained on data about our behaviors can bake our in-group behavioral bias into the choice sets they they construct for us. The result is that in-groups are doubly favored: the algorithm includes more of the in-group into our choice set; and we continue to favor them in choosing from that set. The in-group is more favored when algorithms aid us in our choices than when we choose on our own. Even people whose behavior is otherwise unbiased themselves can wind up making biased algorithmically-assisted choices because the bias of others leads algorithms to over-represent in-group choices among the set of choices considered. In other contexts, we have seen how algorithms perpetuate our human biases. Here, the combination of algorithmically constructed choice sets and preference-behavior wedges leads to an even more pernicious outcome: algorithms {\em exaggerate} our biases.  

We consider how this mechanism plays out in the context of the largest social media platform in the world: Facebook. Specifically we study two algorithms on the Facebook platform for ingroup bias, which we define as over-representation of in-group items in algorithmically-constructed choice sets relative to our own preferences. The main algorithm we study is Newsfeed, a central part of the Facebook experience that takes all the posts of a user's friends then prioritizes them and displays them in ranked order when we log-on. We also study the People You May Know algorithm (PYMK), which helps users construct their social networks by suggesting a list of possible friends from the pool of other users. 

In experiments with US users, we find that Newsfeed ranks own-race posts higher than other-race posts, relative to user's preferences. <Insert some striking #s>. PYMK, on the other hand, shows no ingroup preferences. Both these results hold even when we control for factors such as number of mutual friends. 

The differential evidence of in-group bias across the two algorithms is consistent with the psychology of the different types of choices being informed. Algorithmic behavior differs in these two contexts because human behavior differs in these two contexts. The decision about whether to interact with a post happens quickly and frequently - do we dwell on it and read a little longer, click on a comment to expand on it, choose to ``like'' or otherwise react to it, etc. There is not a lot of time spent deliberating. In contrast, we might spend more time on deciding whom to friend, ask ourselves if this is what we want to be doing, a person whose posts we may want to see and whom we want seeing our own musings and pictures. On existing measures of deliberateness, users also perceive these intuitive differences about their own behavior across these two contexts. [ONE NUMBER FROM OUR STUDY?] Newsfeed then is trained on data where there is a bigger gap between preferences and choices; and in fact, it reflects a larger ingroup bias. We cannot know for certain all the reasons why these two algorithms differ in their biases. For that, we would need a great deal more access to how they were trained. Still, psychology provides an intuitive explanation. 

Finally, to assess the generality of these findings, we repeat the experiment in India. Aside from being an interesting context given its sheer scale (1.35 billion people), it also enables us to measure a very different kind of in-group beyond race: religion, specifically Hindus and Muslims. Despite the very different setting and in-group definition, we find almost exactly the same qualitative pattern as before: Newsfeed shows in-group favoritism of a similar magnitude to what we see in the US, and yet again PYMK shows no statistically detectable in-group favoritism. 


\section{A Simple Model}


A user indexed by $j$ faces an ordered list of items each indexed by $i$, such as posts to look at, friends to add, resumes or application files to consider, etc. The user encounters the items in a ranked order $r(i)$. Absent any algorithm, there will be some natural ranking (e.g. by order of arrival, alphabetical, etc.), $r^n(i)$, but since the original index $i$ was arbitrary, we can assume that $r^n(i) = i$. The user makes a decision about each item they encounter (engage with the post or not, friend this person or not, interview this candidate or not). Because the user cannot view every single candidate item available, instead they view the first $S$ items.\footnote{$S$ could be exogenous and random, or the endogenous solution to an optimal stopping rule. The way $S$ arises is not important, what is important is that the user will (potentially) only look at a subset of all available items, and that they will consider the items in the ranked order they are shown.}



Each item $i$ brings utility $u_{ij}$ to user $j$. Every item is also either associated with one's own-group or not--- we denote this by a binary variable $g_{ij}$, where $g_{ij}=1$ indicates an out-group item. Users ideally would like to choose all positive utility options. The user only views a limited set of information before deciding to act on an item. Thus, while the user would like to choose an option if $u_{ij} >0$, their actual choice may differ from that. In particular each option gets chosen with probability equal to 
$$c_{ij} = (1-b_jg_{ij})c(u_{ij})$$


where the randomness reflects the noise that arises from assessing options from limited information. Here $c(\cdot)$ is a monotonically increasing function that reflects that users tend to be attracted to items they like. The $(1-b_jg_{i})$ term reflects the potential for bias in a user's behavior. At $b=0$ an item's group has no effect on the user's choice. If $b >0$, outgroup items are less likely to be chosen, holding utility for the item constant. 

We refer to $b$ as the behavioral bias in user choices: the bias that can arise in behaviors (actual choices) $c$ above and beyond one's utility $u$. The user may also exhibit outroup bias directly in their utility, but that is not our focus. Our point instead is that, in certain contexts, our choices, $c$, can reflect outgroup bias \emph{beyond} whatever bias there is in the user's preferences, $u$.\footnote{Of course, biased preferences are problematic but in all our analyses and comparisons preferences are held constant. As such our results hold irrespective of however much a given person's preferences favor (or not) ingroups.} 

Now assume an algorithm produces a ranking of items for users, $r^a$. It is trained on a large data set of past choices from users where the items were ranked naturally.\footnote{As a result, our results are not due to a feedback loop in which algorithms are trained on data that are themselves algorithmically generated. Practically, in contexts such as these there exists well-developed tools for addressing that problem.} The goal of all such algorithms is to extrapolate--- users face new choices and the algorithm must find similar choices in the past to produce the ranking. To model this we assume that item $i$ has characteristics $x_i$ and that user $j$ has characteristics $w_j$. We then assume that using data across all items and users, the algorithm estimates for each item a user's propensity to choose a particular action if they were to see it: $\bar{c}_{ij} = E[c_{ij}|x_i,w_j]$, which it then uses to rank order items. Note that $u$ does not enter this expectation--- the algorithm can only make predictions based on the data available to it, that is, our \emph{behavior} not our underlying utility. We have modeled a very friendly scenario for the algorithm: infinite data unpolluted by the algorithm's own ranking. Despite this, a bias emerges. 

\todo[inline]{(DD) The bias emerges from taking this expectation, but it's not clear to me how we skipped to this expectation being central. I think formally the problem that FB is trying to solve (via the algorithm) is

\begin{equation}
\begin{aligned}
\min_{T} \quad & L(T, u)\\
\textrm{s.t.} \quad & \chi(T, u, m, n) \leq \bar{\chi}\\
\end{aligned}
\end{equation}

(dropping i subscripts here for ease)

In words, FB wants to choose some total order T over the set of n items such that some loss function L(T, u) is minimized subject to some constraint $\chi$. If each item's $u_j$ is independent of it's rank relative to other items, e.g., there are no context effects (Is it even okay to assume away order effects this way?), then this problem reduces to

\begin{equation}
\begin{aligned}
\min_{r^a} \quad & L'(r^a, u)\\
\textrm{s.t.} \quad & \chi'(r^a, u, m, n) \leq \bar{\chi'}\\
\end{aligned}
\end{equation}

where $\chi'$ is the cost or difficulty of arriving at a given rank $r^a$. \\ \\

First, let's assume that $\chi$ is constant, we can ignore it and I see that $\bar{c_{ij}} = E[c|x_i, w_j]$ is \emph{one} solution, but I think any function f that has the same ordinality as $\bar{c}_{ij}$ would also be a solution and FB would be indifferent between them. Let $F$ be the set of all functions with the same ordinality as $\bar{c}_{ij}$ (including itself). Then the eligible function class for the algorithm would be the set $F$ not just the function $\bar{c}_{ij}$. Intuitively, it isn't necessary for the algorithm to actually calculate the probability of any item being clicked, only something that allows it to order items in the same way that the true probabilities would. This motivates an intuition for algorithms constructing/inferring preference relations or similarity scores, of which there are many. For example, I can take a user's past behavior and sort new items based on how similar new items are to previously liked items. The algorithm can do this without any information on the user and without any explicit expectation over clicks/behavior, thus removing the channels for the bias we propose. Returning to $\chi$, if we assume it grows with S, m, and/or n, it is easy to construct situations where $\bar{c}_{ij}$ no longer solves the problem and other, cheaper options in $F$ are optimal.

Does this make sense? If so, should we prove for the more general case of algorithmic learning, i.e. the algorithm searches through $F$ or do we sidestep that and circumscribe our findings to the type of user-item statistical learning ($\bar{c}_{ij}$) described above?

See \href{https://developers.google.com/machine-learning/recommendation/content-based/basics}{here}, \href{https://en.wikipedia.org/wiki/Preference_learning}{here}, and \href{https://en.wikipedia.org/wiki/Learning_to_rankfor}{here} for examples of elements (functions) in $F$.}

To understand this bias, we define $K_{ij}(r,g)$ to be the rate at which user $j$ chooses item $i$ when items are ranked according to rule $r$ and if that item were affiliated with group $g$. It captures the idea that lower ranked items are less likely to be chosen because there is a higher chance they will not be considered at all. Notice this definition involves a specific counter-factual--- if a given option were associated with either in-group $g=0$ or out-group $g=1$, how would that affect how the algorithm ranks it and then whether it is chosen or acted on.  

We then define the bias of a ranking rule $r$ to be:
$$\beta_{ij}(r) = 1- \frac{K_{ij}(r,g=1)}{K_{ij}(r,g=0)}$$
This tells us for a given post how often would a user choose to act on it if it were in-group versus if it were out-group. For simplicity we will write $\beta_{ij}^h = \beta_{ij}(r^n)$; that is, human bias is the bias in choices that emerges in the natural ranking. We write $\beta_{ij}^a = \beta_{ij}(r^a)$ as the bias in choices that emerges for \emph{algorithmically} ranked posts. 
It is easy to see that for the natural ranking $\beta_{ij}^h = b_j$.\footnote{Let $s_{ij}$ be the ranking of the post. In general, the probability an item is acted on is the probability you would act on the item if you saw it times the probability you see it = $c_{ij}Pr(s_{ij}<S)$ (assuming the ranking and the probability of the choice independent of the ranking of independent). In the natural ordering, $Pr(s^h_{ij}<S| g=1)=Pr(s^h_{ij}<S|g=0)$. And thus $K_{ij}(h, g=1)=(1-b_j)c(u_{ij})$ and $K_{ij}(h, g=0)=c(u_{ij})$ it follows then that \beta_{ij}^h=1-\frac{(1-b_j)c(u_{ij})}{c_u_{ij}}=b_j } We show the following proposition about algorithmic bias: 

\begin{prop}
As long as a user has some bias $b_j >0$, their behavior will be even more biased under algorithmic ranking: 
$$\beta_{ij}^a > \beta_{ij}^h.$$ 
Even if a user has zero bias ($b_j=0$), then they will still act in a biased manner with algorithmic ranking: $$\beta_{ij}^a > \beta_{ij}^h =0$$ 
as long as other users like them have some bias $E[b_l|w_l=w_j] >0$. 
\end{prop}

\textbf{Proof outline} (presumably for an appendix, to the extent necessary at all. Requires notation from footnote, $s_{ij}$=ranking of post, $S$=how many you look at. The independence assumption implicitly used here may need some thinking though, hence why this is an outline)

$K_{ij}(a, g=1)=((1-b_j)u(c_{ij})) Pr(s_{ij}^a<S|g=1)$

$K_{ij}(a, g=0)=(u(c_{ij})) Pr(s_{ij}^a<S|g=0)$

The first terms become $b_{ij}$ as before, thus $\frac{K_{ij}(a, g=1)}{K_{ij}(a,g=0)}=b_{ij}\frac{Pr(s_{ij}^a<S|g=1)}{Pr(s_{ij}^a<S|g=0)} $

The algorithms sorts items according to their predicted probability the user would choose them. If the user has $b_j>0$ then $(c_{ij}|g=1)<(c_{ij}|g=0)$ and thus the algorithm's $E(c_{ij}|g=1, x_i, w_j) <E(c_{ij}|g=0, x_i, w_j) $ which implies outgroup posts will be ranked lower. If outgroup posts are lower, then $Pr(s_{ij}^a<S|g=1)<r(s_{ij}^a<S|g=0)$ which implies $\beta_{ij}^a=1-b_{ij}\frac{Pr(s_{ij}^a<S|g=1)}{Pr(s_{ij}^a<S|g=0)}>b_{ij}=\beta_{ij}^h$.  And something similar with the fact that the algorithm uses expectations of people like you.

\todo[inline]{(DD): Will CS folks be sensitive to the fact that our claim about recommendation systems is very general, but this proposition is specific to user-centric learning? For example, the algorithm could be item-centric and simply recommend items that are "similar" to what a given user has already clicked. If the similarity score is unsupervised (or supervised by objective labels with no bias, e.g. "puppy", "house", "blue", etc), this approach would be blind across users and may not even reflect bias within user I think.}
\todo[inline]{(JL): Diag, I think what you're worried about above is ruled out by the way we've set this up. the thing being predicted is behavior, i.e. the user's choice, rather than an objective label like "puppy". The algorithm at the very least is able to see each user's past history of choices. I think the algorithm won't need very many user characteristics at all for the rest of the model to fall out, that is for bias from biased users to propogate to unbiased users - even something very basic like ZIP code would do it for bias by race, religion , political preference etc, I think?}

\todo[inline]{(DD): Jens, sorry I mean something different. Take a look \href{https://developers.google.com/machine-learning/recommendation/content-based/basics}{here}. I think your response is about the LHS of the algorithm. My comment is about the RHS. We assume user features enter the RHS, which is how the bias propagates across users. But its possible (and often optimal) to only have item features on the RHS. Then the question becomes whether those features are able to pick up social categories. I believe this is all outside of the model.}

The algorithm biases behaviors because it adjusts the order in which options are seen and because users do not look at literally every choice---due to limited bandwidth, they only consider the first $S$ choices. So for any user's choice of S, later ranked items are less likely to be seen or acted on. The algorithm's ranking $r_{ij}^a$ depends on how users like $j$ behave (i.e users where $w =w_j$ click on items where $x=x_j$). If ingroup options are ranked higher, they are more likely to be seen and simply by virtue of that have a higher chance of being acted on. Of course, conditional on being seen, the user themselves can have bias in their choice. As such the algorithmic bias is {\em added} to any human bias. It exaggerates the problem: biased choices from an already biased choice set. Moreover, notice that in forming its estimates the algorithm uses data on all users. It learns the average bias of users that look similar. As such even an unbiased user can end up with a biased choice set---and hence biased choices. 

Because the algorithm learns from the behavior of the population of users, and averages those behaviors and feeds back the resulting ranking to individual users, the algorithm takes bias by some users and creates bias in the behaviors of every user.


%%%%%%%%%% STUDY DESIGN %%%%%%%%%%
\section{Study Design} 

We advertised for study subjects who were Facebook users and were willing to participate in a Zoom-based interview. Study populations were accessed through the University of Chicago Booth School of Business Center for Decision Research (CDR), the Harvard University Decision Science Laboratory (HDSL).  Each of these sites maintains a participant pool drawn mainly from the local population.\footnote{CDR recruits participants through posts to Facebook, Twitter, and other social media. HDSL recruits from the local Cambridge community.} Further details on our sample recruitment protocol can be found in Appendix~\ref{app:materials}.

Subjects were first asked to complete a survey asking about their demographic characteristics and basic Facebook usage patterns. Subjects were then asked to log in to their Facebook account and share their screen. Data was collected in several waves, and different subjects had different data collected depending on the wave they participated in:
\begin{itemize}
    \item For all participants, enumerators captured the Newsfeed algorithmic ranking, $r^a$, and information about each of the first 60 posts in the user's Newsfeed (N$=$\FullNFUSSurveySampleSize{}).
    \item A subset of Newsfeed subjects then participated in a similar data collection about the first 60 friend recommendations from the PYMK algorithm (N$=$\PYMKUSSurveySampleSize{})
    \item A different subset then participated in data collection about their 10 most recent interactions with Newsfeed posts on Facebook (N$=$\RecentInteractionsSampleSize{})
    \item A final subset had no further data collection (N$=$\NoFurtherSampleSize{})
\end{itemize}
Which data collection a subject participated in was determined by the date and location of data collection---a complete description of this wave structure of the data collection is in Appendix~\ref{app:materials}. In our main analysis we will use the full samples available for each result - that is we will present results for Newfeed in the full sample for which we collected Newsfeed data, results for PYMK in the full sample for which we collected PYMK data, and results for interactions in the full sample where we collected interaction data. In Appendix~\ref{app:tab_fig} we will also present Newsfeed results using only the subsample for which we also have PYMK information.

For the US study we define in-group and out-group by whether the user and the Newsfeed poster (or potential friend being recommended) belong to the same race/ethnic group. To avoid priming study subjects about the topic of race, and because of time constraints on our data collection with each subject, we did not ask subjects themselves to report the race/ethnicity of posters on Newsfeed. Instead we asked our enumerators, who could see the subject's Facebook account through screen sharing, to record their perception of the  race/ethnicity of the posts from people (not companies) in each of the first 60 total posts and first 60 friend recommendations. To measure the subject's own race/ethnicity, we asked subjects to self-report using the seven race and ethnicity categories from the US Census, where subjects can check as many boxes as they like. We also asked the enumerators to record their perception of the race/ethnicity of the subject before the Facebook data collection began. In Appendix ~\ref{app:tab_fig} Table X, we show that there is overwhelming agreement between the enumerator perception of the subject's race/ethnicity and the subject's self-report (RA's perception matches the subjects self-identification exactly 85\% of the time\footnote{This is under a very strict definition of match, and given that subjects often chose more than one race but RA's rarely did, a looser definition of match gets an even higher concordance rate.}).

\todo[inline]{AA: The first sentence of the next paragraph used to say: "While the algorithm is limited to seeing user behavior, in our survey we can get direct measures of utility or user explicit preferences about items that the algorithm cannot." But given the conversations last week about the fact that facebook \textit{does} collect this information for some people and the implications of that for our survey, which I think will now be covered in the intro/model, I am going to soften this language. }

We also collect direct measures of utility, measured by explicit preference for Newsfeed posts and by familiarity for PYMK friend recommendations. We asked subjects to report for each of the posts from people amongst first 60 total posts they see on Newsfeed: “There are more posts than Facebook can possibly show you. How would you rate this post on a scale from 1-7 where 1 means ‘can skip’ and 7 means ‘definitely want to'''. For those that participated in the PYMK data collection, for the first 60 friend recommendations on PYMK they are asked: “How familiar are you with this person on a scale from 1-7?” where 1 is not familiar and 7 is very familiar.''. The enumerators also record ancillary information about the Newsfeed posts such as how long ago it was posted, whether the post is made by person or company, and whether it was to a Facebook group. For PYMK recommendations, enumerators recorded additional information like how many mutual friends that each friend recommendation has with the subject.

For a subset of subjects we also measured not just preferences for posts, but also their recent behavior on Facebook (N$=$ \RecentInteractionsSampleSize). Specifically we recorded the 10 most recent posts on Newsfeed that the user had some \emph{interaction} with (``liking'' or choosing another reaction or commenting), and exactly what action they took. Enumerators then also recorded the perceived race/ethnicity of the poster.\footnote{The algorithm presumably has access to a wider range of behaviors than this, such as how long the user lingered on a post, whether the user clicked to expand on the post text or comments, whether the user watched a video and how much of the video was watched, etc. Given the constraints on our data collection, interacting with posts was the most feasible measure of actual user behavior on the network.} 

Central to our model is the idea that behavioral bias by humans can create algorithmic bias in these types of settings. Previous research suggests behavioral biases are most pronounced when behavior is not guided by deliberate thought.\footnote{See for example \citet{payne2002best}, \citet{lueke2015mindfulness} and the studies reviewed there}. To understand whether and how the Newsfeed and PYMK algorithms differ in the degree to which users make considered decisions, we carried out a separate survey on Prolific to measure the amount of cognitive effort, deliberation, and time spent making choices to interact with posts on Newsfeed versus add a friend from ``People you May Know'' (N$=$\ProlificSampleSize). We draw on existing measures in the literature about, for instance, how well the subject could explain their choices, how much ``mental'' effort they say they put into the behavior, whether the decisions are based on gut feelings or careful consideration, and how much time they usually spend (in seconds) making the decision (CITATIONS TO THIS EXISTING LIT). For more details on our specific measures see Appendix~\ref{app:survey}.

Note that subjects were not made aware that this study was about in-group biases or race, and the exact data being recorded by the enumerator was unknown to the subject. For \StudyAboutSampleSize{} subjects, at the end of the survey we asked ``What do you think the purpose of this study is?'' Not one mentioned race, gender, or other indications of in-group biases.\footnote{Similarly, RAs were not told the purpose of the study, though they were of course aware they were collecting information on race.}

We also collected an additional wave of data from India (n$=$\NFnPYMKIndiaSurveySampleSize{}), using same-religion as an in-group definition. For more details on this sample and the results see Section X and Appendix~\ref{app:india}.

%%%%%%%%%% RESULTS %%%%%%%%%%

\section{Results}

\subsection{Summary Statistics on US Study Sample}

Table~\ref{tab:sumstats_p} shows summary statistics on our subject pool for the US subjects for whom we collected Newsfeed information (N=\FullNFUSSurveySampleSize{}), subjects for whom we collected PYMK information (N=\PYMKUSSurveySampleSize{}), and subjects for whom we collected recent Newsfeed interactions (N$=$\RecentInteractionsSampleSize).  Our subjects are on average in the mid-20s (sd$=$9.186, range$=$18 to 69).  A large fraction of our sample check Facebook at least weekly. Compared to all US adults, our samples tend to have a higher proportion Asian, female, and people with a four-year (bachelor's) college degree. In the appendix we show that re-weighting the data to demographically match the rest of the US with respect to gender, race and ethnicity yields qualitatively similar findings. (CHECK) 

The preference and familiarity ratings we collected vary substantially between subjects in terms of both the mean rating and the variance of these ratings, which is presumably due to some combination of differences in the distribution of types of posts across subjects and differences across subjects in how they interpret and use the Likert scale. So rather than rely on the raw preference responses, which are hard to compare across subjects, we create within-subject normalized preference rankings to use in our analysis. For completeness we show in the appendix that our main results also hold when we analyze the data using each study subject's actual raw Likert scale rating for each post instead. (THAT'S TRUE RIGHT?)

From these subjects we collected data on a total of \numposts{} Newsfeed posts, of which \numhposts{} were from people (not companies) and thus included in our analysis sample; \numrecs{} PYMK friend recommendations; and a total of \numinteract{} interactions of which \numhinteract{} were interactions with posts from people (not companies).\footnote{Even the total numbers do not add up exactly to \FullNFUSSurveySampleSize{}$\times$60 and \PYMKUSSurveySampleSize{}$\times$60 because we were not always able to capture a full set of 60 for each participants. For 80\% of subjects were able to collect 60 posts on Newsfeed, amongst those we did not collect the full 60 on average we were able to collect 40. On PYMK we were able to collect the full 60 recommendations for 94\% of the sample.  This could happen due to multiple reasons: although generally one is able to continuously scroll through the Newsfeed for some participants the algorithm would not show more than X$<$60 posts; because the Newsfeed refreshes each time you go go the home page, if someone accidentally clicked on a post then tried to go back the Newsfeed would refresh and we would stop data collection if more than a few posts had already been collected; sometimes there are internet problems that stop data collection prematurely, or subjects wanted to stop because the collection was taking too long.} Table~\ref{tab:sumstats_o} shows summary statistics for these posts, recommendations, and interactions.  About 47\% of posts in Newsfeed are from people of the same race. 58\% of friend recommendations are of people of the same race.

\subsection{Newsfeed}

We start with results for the most widely used algorithm on Facebook, the one each user encounters as soon as they log-in: Newsfeed. It is useful to compare the algorithm's ranking to some natural alternative,$r^n$, such as reverse chronological order---which is how Newsfeed originally ranked posts until October 2009.\footnote{See \url{https://wallaroomedia.com/facebook-newsfeed-algorithm-history/}.} Using the data we collected on how long ago the post was created, we can compare what a reverse chronological ranking of the first 60 posts would have looked like (shown on the x-axis in the `heatmap' presented in Figure~\ref{fig:nftime_hm}) compared to the actual Newsfeed algorithm rankings (as shown on the y-axis of the `heatmap' Figure~\ref{fig:nftime_hm}, with the top-ranked posts at the top). The algorithm does tend to sort the most recent posts closer to the top, as indicated by the concentration of data points in the upper right corner. But the Newsfeed algorithm is also clearly taking other things into consideration and doing \emph{something} beyond this natural reverse-chronological rank ordering, since the data are very clearly not all concentrated along the 45 degree line in the heat map.

Figure~\ref{fig:nfpref_hm} shows that the Newsfeed algorithm is to some degree successful at doing what Facebook reports the algorithm aims to do, which is to show users posts that they are most likely to be interested in.\footnote{See \url{https://www.facebook.com/notes/facebook-app/interesting-news-any-time-you-visit/10150286921207131/}.} The y-axis in the figure is the Newsfeed algorithm's rankings of posts, as before, while the x-axis is now where each post falls within the user's self-reported preference distribution (where the posts the study subject is most excited to see are at the right; we report everything in percentiles within the user's self-reported preference distribution). The Newsfeed algorithm does succeed in putting posts that are particularly high up in the user's preference ranking towards the top of the ranking.

To the right of each heatmap, in Figures~\ref{fig:nftime_s} and ~\ref{fig:nfpref_s} we present this data in a slightly different way, showing the flows from chronological or preference quartiles (at left) into quartiles of the algorithmic ranking (at right). In Figure~\ref{fig:nftime_s} we can see that the most recent posts are disproportionately likely to show up at the top of the Newsfeed ranking (41\%), but the other chronological quartiles are distributed fairly evenly across the Newsfeed algorithmic rankings. In Figure~\ref{fig:nfpref_s} we see similarly that posts in the top quartile of the preference rankings are more likely to end up in the top of the Newsfeed algorithmic ranking (32\%). So the algorithm does take chronology into account, and successfully learns something about underlying preferences, but is also taking other factors into account as well.

Figure~\ref{fig:nf_bygroup} shows us a different aspect of the Newsfeed algorithm: It disproportionately puts in-group posts higher in the algorithmic rankings (the heat map at the right) relative to out-group posts (the heat map to the left). This is pronounced enough to be able to see visually in the raw data. Panel~\ref{fig:nf_bygroup_em} gives a better sense of the relevant magnitudes. We sort posts into groups of 10 based on their Newsfeed algorithm ranking, with higher-ranked posts at the top of the y-axis in the figure. For each group we report the share of posts that are in-group minus the overall share of all the user's posts that are in-group, which we term the "excess mass of in-group posts." The posts at the top of Newsfeed's rankings are well above base rate in the share that are in-group (by about 10\%), while posts at the bottom have a share in-group well below base rate (by about 8\%). \todo{AA: Check these numbers}

A natural question is whether Newsfeed is simply giving users what they want; does the up-ranking of in-group posts simply reflect user preferences? No, at least not according to explicit preferences. Figure~\ref{fig:nf_main} shows that in-group posts are disproportionately concentrated at the top of the Newsfeed algorithm's rankings conditional on user self-reported preferences. Figure~\ref{fig:nf_line} shows this a few different ways. The median of each user's 60 posts of course shows up right in the middle at ranking 29. In panel A we show how far above this median does the average in-group versus out-group post show-up for each of the 4 quartiles of user preference ranking. The first thing to note is that the ingroup posts always have large ranking boosts above median than outgroup posts. While this is true all along the spectrum of user preferences, it is particularly true in the lowest quartile where the average ingroup posts gets a 1.5 \todo{AA: Check numbers} ranking-slot boost above median on average, while the average outgroup post in this quartile gets a ranking decrease of about 0.5 a ranking-slot below median on average. Another way to see the magnitude of the race difference is that an outgroup post in the 3rd quartile of the user preference ranking is ranked at a similar level on average as an ingroup post in the \textit{second} preference quartile. 

Panels B-D show analogous figures, with the Y-axis now instead representing the probability a post shows up in the top-5, top-15 or top 15. Regardless of how we measure the ranking boost, in-group posts are consistently ranked higher than out-group posts conditional on user self-reported preferences.

Figure~\ref{fig:nf_rankingboost} to the right shows this in a different way. The x-axis again is posts grouped by user preference quartile. For each post we calculate the difference between it's actual Newsfeed ranking and where the post would fall if ranked in reverse chronological order, then show the average of that for each preference quartile on the y-axis, separately for in-group and out-group posts. For the posts that users are least interested in (preference quartile 1), in-group posts show up about 2 slots higher in the Newsfeed actual ranking than are out-group posts. For the highest-ranked posts (quartile 4) the in-group versus out-group gap relative to chronological ranking is about one slot higher for in-group posts. Put differently the in-group posts that subjects report the least interest in (quartile 1) have about the same average rank-ordering by the Newsfeed algorithm as out-group posts in the next-higher user-preference quartile 2.

Interestingly, normalized self-reported preferences about posts do not reveal signs of much in-group bias. Table~\ref{tab:sumstats_o} shows that differences in quartile rankings of normalized preferences between in-group and out-group posts are very similar, and none of the differences are significant at the 5\% level. In Appendix Figure~\ref{fig:prefcdf} we also show a cdf of the normalized preference which lie basically on top of each other.\footnote{Appendix Table X or Figure Y shows there is a difference in raw likert scale rankings by ingroup versus outgroup, however this reflects that fact that Black participants tended to have more ingroup friends and be more likely to use likert pref rankings 5, 6, and 7. The normalized preferences take into account within subject differences in the use of the likert scale.} 
\todo[inline]{AA: should we talk about above paragraph after sum stats instead? JL: I thought the sum stats section gave too much of the story away in a sort of bland oh-by-the-way way, so cut all that out so we could build it up and build it out a bit more? Jens}

\subsection{PYMK}

Figure~\ref{fig:pymk} mirrors Figure~\ref{fig:nf} but for the PYMK algorithm. Here we consider the natural ranking, $r^n$, to be the number of mutual friends the subject has with the recommended friend divided by the number of total friends the subject as - that is what percent of the subjects Facebook friend do they have in common with the recommendation. \todo[inline]{CHECK - is that true? is that how we did this there? Jens} As shown in Figures~\ref{fig:pymkfriend_hm} and Figures~\ref{fig:pymkfriend_s} the algorithm is up-ranking people with whom subjects have relatively more mutual friends in common with. The PYMK algorithm is similarly picking up on user's self-reported familiarity with the potential friend as Figures~\ref{fig:pymkpref_hm} and Figures~\ref{fig:pymkpref_s} show, as those that users report being more familiar with are also more likely to show up higher on the recommendation list. As with Newsfeed, the PYMK algorithm is partly reflective of what seems like a reasonable natural ranking (friends in common), and is partly reflective of user's self-reported preferences (the degree to which subjects know the people being recommended), though clearly other factors are at play as well with the algorithm.

In contrast to the Newsfeed algorithm, however, we see little evidence of in-group bias in the PYMK algorithm. The heat maps in Figure~\ref{fig:pymk_bygroup_hm} show how recommendations are ranked by the PYMK algorithm separately for in-group and out-group recommendations. Unlike with Newsfeed, there is little visual difference in rankings by in-group versus out-group status. Figure~\ref{fig:pymk_bygroup_em} to the right shows this a different way, where we rank PYMK recommendations by the algorithm's ordering, put them into groups of 10 (y-axis) and show the excess mass of in-group recommendations. The highest-ranked recommendations if anything have negative excess in-group mass (the share in-group is below base rate), and more generally there is no consistent relationship between PYMK ranking and in-group share.

Figure~\ref{fig:pymk_line} shows the similar results as Figure~\ref{fig:nf_line}: how many slots average does a friend recommendation move above the median by familiarity quartile and in-group versus out-group in Panel A and the probability of being in the top 5, 10, or 15 recommendations in B-D. Here we see that the in-group and out-group lines lie nearly on top of each other with no real difference in how the algorithm ranks in-group or out-group recommendations conditional on familiarity quartile. And Figure~\ref{fig:pymk_rankingboost} similarly shows that while yes potential friends you are more familiar with tend to get a higher average boost in ranking, these is little difference in the mean ranking boost that posts get above the median ranking (30) by in-group and out-group. 

\subsection{Explaining the Difference}
Why do we see evidence of out-group bias with the Newsfeed algorithm but not with PYMK? Our model suggests one hypothesis: Algorithms rely more on user behavior than user preferences, and we are more likely to see a behavior-preference difference towards more in-group bias in cases where the behavior is relatively less deliberate. We expect user behavior with Newsfeed posts, which is compared to PYMK friend recommendations more frequent and lower-stakes, to be less deliberate than user behavior to accept PYMK friend recommendations. 

Figure~\ref{fig:auto_effsize} shows that for each of the nine measures of how deliberate people's interactions are with either the Newsfeed or PYMK algorithm, users report higher levels of deliberation with PYMK than Newsfeed. In this figure we take the difference between the measure of deliberation in interacting with PYMK and that of interacting with Newsfeed, then divide by the pooled standard deviation of the measure (so units are reported in terms of Cohen's d on the y-axis). The advantage in favor of PYMK in terms of how deliberate the behavior is range from about 0.05 standard deviations (for inattention) to around 0.5 standard deviations (carefulness). When we do a principal components analysis across the measures and compare the first principal components, the PYMK advantage over Newsfeed in terms of being more deliberate equal to around 0.4 standard deviations. A simple composite index of the standardized measures suggest a PYMK deliberation advantage of just over 0.2 standard deviations. Figure~\ref{fig:auto_binaries} shows the result in natural units for the two binary measures, while Figure~\ref{fig:auto_likerts} shows the results in natural units for 7 questions asked on a 7-point likert scale measure as well as the simple average composite score. And Figure~\ref{fig:auto_cdf} shows the CDF for responses to the one continuous measure, time it takes the study subject to make a decision (in seconds).

In sum, study subjects report spending more time and make decisions more deliberately for choosing friends on PYMK relative to interacting with Newsfeed posts. This is consistent with our hypothesis that the behavioral wedge (relative to preferences) in the direction of in-group bias should be smaller for PYMK than Newsfeed.

\subsection{Interactions/Behavior}
Our model not only gives us a way to understand the source of algorithmic bias, but also yields a prediction about the magnitude of the consequences. In our model the biased algorithm compounds the user's own bias (or bias of users like her) by showing the user a choice set that over-represents in-group items. So the magnitude of the bias in user's \emph{behavior} at the end of the day should be even larger than the bias we see in the algorithm rankings, given that they choosing amongst this already biased set. To explore this hypothesis we collected data on the last 10 \emph{interactions} people had with Newsfeed posts: reactions and comments. This is not a perfect measure of user behavior with Newsfeed, since there are other behavioral dimensions that we cannot mention in our setting (like the time the user had spent looking at the post, etc.) We present results here just for the sub-sample of respondents for which we have this behavioral measure.

Figure~\ref{fig:behavior} shows that the share of Newsfeed posts that the user has interacted with that are in-group is indeed larger than the share of posts that are in-group among those that the algorithm ranks towards the top of the user's Newsfeed. The confidence intervals here are somewhat large but the excess share of in-group posts among those the user interacts with (user behavior) is between 50\% and 100\% higher than the posts the algorithm puts in the user's top 5, 10 or 15 of the Newsfeed ranking. The algorithm is not merely reflecting our own behavioral biases back to us, it is amplifying those biases in our behavior.

\subsection{Additional Results from India}

Part of what makes Facebook an interesting test-case for our theory about algorithmic bias is its massive scope. Billions of people around the world regularly use Facebook and rely on its algorithms. To show that our results are quite general, and not specific either to the US context or to defining out-group bias along the lines of race specifically, we replicated our analysis to test for bias in another massive Facebook market: India. We recruited 200 study subjects via the Ashoka University Centre for Social and Behaviour Change (CSBC). In this sample in-group/out-group is defined by religion rather than by race, but otherwise the study proceeded identically as in the US. In Figure X we replicate Figure~\ref{fig:rankabovetime} with data from the sample from India and again see evidence of out-group bias by religion. Figure Y repeats the exercise for PYMK, once again (as in the US context) showing no statistically significant evidence of out-group bias. (Additional results for this India sample are presented in the Appendix.)


%%%%%%%%%% CONCLUSION (OR DISCUSSION) %%%%%%%%%%
\section{Conclusion}
We have seen how algorithms can magnify our biases. They are problematic in another way. When people recognize a gap between their preferences and their choices, they can do something about it. They can adjust their environments and how they approach the choice to bring these two in line with each other. But the algorithm blindly learns from behavior without access to preferences. Since it, not us, constructs the choice set, it impedes the opportunity for us to learn the problem, which can make the gap more persistent.

\pagebreak
\clearpage
\singlespacing 
\bibliographystyle{chicago}
\bibliography{references}


%%%%%%%%%% TABLES AND FIGURES %%%%%%%%%%


%%%%%%%%%%
%%% US: Summary Statistics
%%%%%%%%%%


\begin{table}[]
    \caption{Participant Summary Statistics}
    \begin{center}
    \input{Output/Amanda/sumstats_p.tex}
    \label{tab:sumstats_p}
    \end{center}
\footnotesize \textbf{Note:} 
\end{table}

%\input{Output/Tex/Summary Tables/US NF participants by sample.tex}

%\input{Output/Tex/Summary Tables/US NF PYMK Outcomes.tex}


\begin{table}[]
\caption{Summary Statistics on Collected Outcomes}
    \begin{center}
    \input{Output/Amanda/sumstats_r.tex}
    \label{tab:sumstats_o}
    \end{center}
\footnotesize \textbf{Note:} 
\end{table}

%%%%%%%%%%
%%% US: NF
%%%%%%%%%%

%%% NF Heatmap and Sankey: Chron vs Algorithm and Pref vs Algorithm %%%
\begin{figure}[ht]
\caption{Relationship between Newsfeed Algorithmic Ranking and Chronology and User Stated Preferences }
\label{fig:nf}
    \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/US NF chron rank by nf rank - smooth.jpg}   
        \caption{Relative Intensity of Posts by Chronological Ranking and Algorithmic Ranking}
        \label{fig:nftime_hm}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \begin{center}
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Sankey flows/US NF chronology to actual.jpg}  
        \caption{Flows from Chronological Ranking Quartile to Algorithmic Ranking}
        \end{center}
         \label{fig:nftime_s}
    \end{subfigure}

   \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/US NF norm pref rank by nf rank - smooth.jpg}  
        \caption{Relative Intensity of Posts by Normalized Preference  and Algorithmic Ranking}
        \label{fig:nfpref_hm}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Sankey flows/US NF norm quartile to actual.jpg}  
        \caption{Flows from Normalized Preference Quartile to Algorithmic Ranking}
        \label{fig:nfpref_s}
    \end{subfigure}

\footnotesize \textbf{Note:} Figures (a) and (c) are heatmaps which show that actual Newfeed algorithmic ranking (1-60) on the Y-axis and either chronological ranking (1-60)  or percentiles of normalized preference distribution (1-100) on the X-axis. The heatmaps are smoothed using XXYY. Figures (b) and (d) show flows from quartiles of the chronology ranking or preference distribution (on the left) to quartiles of the actual Newsfeed algorithmic ranking (on the right). The width of the bands represents the proportion in that flow.
\end{figure}



%%% NF Ingroup Posts %%%
\begin{figure}[ht]
\caption{Relationship between Newsfeed Algorithmic Ranking and Ingroup Status of Posts}
\label{fig:nf_bygroup}
    \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/US NF nf rank by ingroup - smooth.jpg}  
        \caption{Relative Intensity of Posts by Ingroup Status and Algorithmic Ranking}
        \label{fig:nf_bygroup_hm}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{Output/Graphs/Audit/Excess Mass/US NF excess mass by ranking group.jpg}  
        \caption{Excess Mass of Ingroup Posts by Newsfeed Algorithmic Ranking}
        \label{fig:nf_bygroup_em}
    \end{subfigure}

\footnotesize \textbf{Note:} DESCRIBE EXCESS MASS
\end{figure}

%%% NF Ranking by preference %%%
\begin{figure}[ht]
\caption{NF Beautiful Results}
\label{fig:nf_main}
    \begin{subfigure}{.5\textwidth} 
        \centering
            % include first image
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Ranking line graphs/US NF all outcomes panel by norm preference by ingroup.jpg} 
        \caption{Something1}
        \label{fig:nf_line}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Misranking relative to expectation/Chronological expectation/US NF by norm preference.jpg}  
        \caption{Something2}
        \label{fig:nf_rankingboost}
    \end{subfigure}

\footnotesize \textbf{Note:} 
\end{figure}

%%%%%%%%%%
%%% US: PYMK
%%%%%%%%%%

%%% NF Heatmap and Sankey: Chron vs Algorithm and Pref vs Algorithm %%%
\begin{figure}[ht]
\caption{Relationship between PYMK Algorithmic Ranking and Chronology and User Stated Preferences}
\label{fig:pymk}
    \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/US PYMK pct friends by pymk rank - smooth.jpg}   
        \caption{Percent Friend Heat Map}
        \label{fig:pymkfriend_hm}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Sankey flows/US PYMK pct friend to actual.jpg}  
        \caption{Percent Friend Sankey (ALL!)}
        \label{fig:pymkfriend_s}
    \end{subfigure}

   \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/US PYMK norm pref rank by pymk rank - smooth.jpg}  
        \caption{Preference Heat Map}
        \label{fig:pymkpref_hm}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Sankey flows/US PYMK norm quartile to actual.jpg}  
        \caption{Preference Sankey}
        \label{fig:pymkpref_s}
    \end{subfigure}
    
\footnotesize \textbf{Note:} 
\end{figure}


%%% PYMK Ingroup Posts %%%
\begin{figure}[ht]
\caption{Relationship between PYMK Algorithmic Ranking and Ingroup Status of Posts}
\label{fig:pymk_bygroup}
    \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/US PYMK pymk rank by ingroup - smooth.jpg}  
        \caption{Two Single Heat Maps by Ingroup}
        \label{fig:pymk_bygroup_hm}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{Output/Graphs/Audit/Excess Mass/US PYMK excess mass by ranking group.jpg}  
        \caption{Excess Mass by Ranking}
        \label{fig:pymk_bygroup_em}
    \end{subfigure}

\end{figure}

%%% PYMK Ranking by preference %%%
\begin{figure}[ht]
\caption{PYMK Results}
\label{fig:pymk_main}
    \begin{subfigure}{.5\textwidth} 
        \centering
            % include first image
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Ranking line graphs/US PYMK all outcomes panel by norm preference by ingroup.jpg} 
        \caption{Something1}
        \label{fig:pymk_line}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Misranking relative to expectation/Mutual friends expectation/US PYMK by norm pref.jpg}  
        \caption{Something2}
        \label{fig:pymk_rankingboost}
    \end{subfigure}
\footnotesize \textbf{Note:} 
\end{figure}

%%%%%%%%%%
%%% US: AUTO
%%%%%%%%%%

\begin{figure}
\caption{Deliberateness}
  \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Experiments/Automaticity/standardized effect sizes.jpg} 
        \caption{Standardized Effect Sizes}
        \label{fig:auto_effsize}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Experiments/Automaticity/bar chart binary measures.jpg} 
        \caption{Just the binaries}
        \label{fig:auto_binaries}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Experiments/Automaticity/composite score and components.jpg}  
        \caption{Likerts (and average composite)}
        \label{fig:auto_likerts}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Experiments/Automaticity/speed cdf.png}  
        \caption{Speed (time to decide in seconds) CDF}
        \label{fig:auto_cdf}
    \end{subfigure}
\label{fig:auto}
\footnotesize \textbf{Note:} 
\end{figure}

%%%%%%%%%%
%%% US: Interactions
%%%%%%%%%%
\begin{figure}[!h]
    \caption{Beautiful Results}
    \label{fig:behavior}
    \centering
    \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Interactions/US preferences reactions and actual rankings above base rate.jpg}
\footnotesize \textbf{Note:} 
\end{figure}

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
%%%%%%%%%% Appendix %%%%%%%%%%
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
\FloatBarrier
\clearpage
\appendix
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\counterwithin{table}{section}


%%%%%% Materials and Methods Appendix %%%%%% 
\section{Materials and Methods}\label{app:materials}

We collected data from XX participants over six sequential waves between March XX, 2020 and October XX, 2020. Over four waves we recruited 466 participants through the CDR (US); In a single wave we recruited 196 participants through HDSL (US); In a single wave we recruited 198 participants through CSBC (India). All waves share the same basic structure in which 1) participant privately completes a self-assessment, 2) RA guides each participant through the Newsfeed while recording information about each post and 3) RA guides participant through some additional data collection.

Data from each participant was collected in a single one-on-one Zoom session with an RA which lasted approximately one hour on average. After the data collection was completed, participants were sent a link to access their payment of \$20 (\$10 in India).

\subsection{Wave Overview}
\begin{itemize}
    \item Wave 1 - CDR, NF + PYMK, 242
    \item Wave 2 - HDSL, NF + PYMK, 196
    \item Wave 3 - India, NF + PYMK, 198
    \item Wave 4 - CDR, NF + Recent Activity, 54
    \item Wave 5 - CDR, NF (connectedness), 120
    \item Wave 6 - CDR, NF (about) + Recent Activity, 50
\end{itemize}

\subsection{Waves 1-3}

Waves 1-3 were nearly identical, each wave was on a different population. Because waves 1 and 2 were in the US, the group membership was based on perceived race, whereas wave 3 in India collected group membership based on perceived religion.

\subsubsection{Part I: Subject Categorization} After joining a Zoom call with an RA, subjects are asked to fill out a Qualtrics survey. In the survey, subjects are asked to describe their demographics and Facebook usage. As a main variable in our study, the assessment of the ingroup is paramount. US subjects are shown the seven race and ethnicity categories used in the US Census and are given the option to check as many boxes as they like. Indian subjects are asked to report their religion.

While the subject fills out the survey, the RA makes her best assessment of the subject’s ingroup (race in the US; religion in India), using up to two categories. Neither the subject nor the RA is aware of the assessment that the other has made. This protocol has the advantage of allowing us to observe how much alignment there is between how subjects self-identify and how they are perceived.

\subsubsection{Part II: Newsfeed} Users open their Facebook account and share their screen with the RA. Then, scrolling sequentially through each post in the Newsfeed, the subject answers exactly one question about each post: “There are more posts than Facebook can possibly show you. How would you rate this post on a scale from 1-7 where 1 means ‘can skip’ and 7 means ‘definitely want to see’.” In addition to recording the explicit preference, the RA assesses and records the perceived race of the poster of the content as well some other details of the post such as how long ago it was posted and whether it was posted to a group. The exact data being recorded by the RA are unknown to the subject. This continues for the first 60 non-sponsored posts.

\subsubsection{Part III: People You May Know} Subjects then navigate to the Facebook recommender for new friends, entitled “People You May Know” (PYMK). The procedure for this section is similar to that in Part II. The subject scrolls down the list and for each recommended user the subject answers one question: “How familiar are you with this person on a scale from 1-7?” In addition to recording the familiarity, the RA assesses and records the perceived race of the recommended user as well as the number of mutual friends. This continues for 60 recommendations.

\subsection{Wave 4}

Wave 4 differed slightly from the waves before it. Parts I and II were identical, but for part III instead of scrolling through the PYMK recommendation, particpants navigated to and scrolled through their 'recent activity' as follows.

\subsubsection{Part I: Subject Categorization} Identical to waves 1-3.

\subsubsection{Part II: Newsfeed} Identical to waves 1-3.

\subsubsection{Part III: Recent Activity} Subjects then navigate to the Facebook activity log, which is sorted in reverse chronological order. The RA instructs the participant to scroll down until identifying the first post with a reaction or comment. Then records perceived race and gender for the identified post. This process repeats for the 10 most recent comments/reactions to posts. As with NF data collection, if the race or gender of a user is not discernible from the post, the RA records the name in a separate list, and comes back to the list after collecting all 10 posts.

\subsection{Wave 5}

Wave 5 sought to collect richer data on the relationship between the participant and the author behind each Newsfeed post. There was no Part III in this wave.

\subsubsection{Part I: Subject Categorization} Identical to waves 1-4.

\subsubsection{Part II: Newsfeed} Users open their Facebook account and share their screen with the RA. Then, scrolling sequentially through each post in the Newsfeed, the subject answers exactly \textit{three} questions about each post: 
\begin{enumerate}
    \item “There are more posts than Facebook can possibly show you. How would you rate this post on a scale from 1-7 where 1 means ‘can skip’ and 7 means ‘definitely want to see’.” 
    \item How well do you know the person who posted this content? (1-7)
    \item How do you know this person? [Family, Friend, Acquaintance, Don’t know personally]
\end{enumerate}

In addition to recording the explicit preference, the RA assesses and records the perceived race of the poster of the content as well some other details of the post such as how long ago it was posted and whether it was posted to a group. The exact data being recorded by the RA are unknown to the subject. This continues for the first 60 non-sponsored posts.

\subsection{Wave 6}

Finally, Wave 6 sought to elicit participant perceptions on the purpose of the study 

\subsubsection{Part I: Subject Categorization} Identical to waves 1-5.

\subsubsection{Part II: Newsfeed}  Identical to wave 5.

\subsubsection{Part III: Recent Activity} Almost identical to wave 4, collecting 30 recent activity items instead of 10.

\subsubsection{Part IV: Study Purpose} RA asks the participant "What do you think this study is about?" and transcribes the answer as close to verbatim as possible.

%%%%%% Additional Tables/Figures Appendix %%%%%% 
\section{Additional Tables and Figures}\label{app:tab_fig}

% CDF US NF Preferences by Ingroup  
\begin{figure}[!h]
    \centering
    \includegraphics[scale=.8]{Output/Graphs/Audit/Stated preferences/US NF cdf norm preferences by ingroup.jpg}
    \caption{CDF}
    \label{fig:prefcdf}
\end{figure}

%%%%%% Prolific Survey Appendix %%%%%% 
\section{Prolific Survey Details}\label{app:survey}

Details on data collection, sample recruitment, and questions for automaticity survey

%%%%%% India Appendix %%%%%% 
\section{India Data Collection}\label{app:india}
Some details about the India Data Collection

\subsection{Tables and Figures for India Data}

%%%% NF %%%%
\begin{figure}[ht]
\caption{NF Ingroup Posts Higher}
\label{fig:nf_bygroup}
    \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/India NF nf rank by ingroup - smooth.jpg}  
        \caption{Two Single Heat Maps by Ingroup}
        \label{fig:nf_bygroup_hm}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{Output/Graphs/Audit/Excess Mass/India NF excess mass by ranking group.jpg}  
        \caption{Excess Mass by Ranking}
        \label{fig:nf_bygroup_em_india}
    \end{subfigure}
\end{figure}

\begin{figure}[ht]
\caption{NF Beautiful Results}
\label{fig:nf_main}
    \begin{subfigure}{.5\textwidth} 
        \centering
            % include first image
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Ranking line graphs/India NF all outcomes panel by norm preference by ingroup.jpg} 
        \caption{Something1}
        \label{fig:nf_line_india}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Misranking relative to expectation/Chronological expectation/India NF by norm preference.jpg}  
        \caption{Something2}
        \label{fig:nf_rankingboost_india}
    \end{subfigure}
\end{figure}

%%%% PYMK %%%%
\begin{figure}[ht]
\caption{PYMK Ingroup Posts Not Higher}
\label{fig:pymk_bygroup}
    \begin{subfigure}{.5\textwidth} 
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Heatmaps/India PYMK pymk rank by ingroup - smooth.jpg}  
        \caption{Two Single Heat Maps by Ingroup}
        \label{fig:pymk_bygroup_hm_india}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{Output/Graphs/Audit/Excess Mass/India PYMK excess mass by ranking group.jpg}  
        \caption{Excess Mass by Ranking PUT ON SAME SCALE AS NF?}
        \label{fig:pymk_bygroup_em_india}
    \end{subfigure}

\end{figure}

\begin{figure}[ht]
\caption{PYMK Results}
\label{fig:pymk_main}
    \begin{subfigure}{.5\textwidth} 
        \centering
            % include first image
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Ranking line graphs/India PYMK all outcomes panel by norm preference by ingroup.jpg} 
        \caption{Something1}
        \label{fig:pymk_line_india}
        \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{Output/Graphs/Audit/Misranking relative to expectation/Mutual friends expectation/India PYMK by norm pref.jpg}  
        \caption{Something2}
        \label{fig:pymk_rankingboost_india}
    \end{subfigure}

\end{figure}

\end{document}
