
Algorithms learn what we like and then curate our choice sets for us. These algorithms, though, are built on a faulty premise: that what we choose reveals what we like. A moment of introspection (and a mountain of research) quickly reveals that this presumption is mistaken. The problem is noteworthy when it comes to discrimination: 
many people who abhor prejudice end up implicitly discriminating. We show theoretically and experimentally how algorithms as currently built not only mirror our biases but in fact exaggerate the bias in our choices. An empirical audit of Facebook's algorithms across two countries reveals large biases consistent with our model.  We argue these problems are endemic across many applications and require a rethinking of how we algorithmically curate choice sets.

We audit two of Facebook's biggest algorithms. Newsfeed, which dictates which posts users see, shows significant ingroup bias: given two posts, equally liked by a user, the one by same race friends are ranked significantly higher. People You May Know, which suggests potential new friends, shows no such bias. We explain this pattern using a well-known fact in behavioral science: ingroup favoritism can inadvertently creep in to our behavior, despite our preferences, when we are acting automatically; and we show that scrolling through posts is a more automatic behavior than choosing who to friend. We build a model that encapsulates these ideas and show how recommender systems - when they ignore these behavioral facts - can not only reflect but in fact exaggerate our implicit biases. We test the model with a controlled experiment where we manipulate the automaticity of choices. Beyond showing significant bias in the one of the largest algorithms in the world - one that dictates the strength of connections between people -- these results suggest a more general caution. Algorithms that purport to learn our preferences do so by looking at our choices: yet our automatic choices often reflect behaviors that do not match our true desires. These problems are especially severe in contexts where behavior is automatic, as seems to be the case with in many online situations. 


 
%Finally, we demonstrate the generality of the underlying psychology and its interaction with algorithmic processes by replicating the results in a very different context, India, with a very different kind of in-group bias, religion.
